---
title: "case rates local authorities"
author: "Vanessa Fillis"
date: "02/06/2021"
output: html_document
---

## Activate packages

At first, we will activate necessary packages. 

```{r}
library(jsonlite)
library(httr)
library(dplyr)
library(zoo)
```

## Import the data

We will import the data of all daily cases in England's local authorities. Data is available for upper-tier and lower-tier local authorities. We need both of them for the map on Datawrapper. 

### Cases Upper-tier local authorities

Cases by specimen date (regional breakdown of cases is only available by specimen date)

```{r}
#' Extracts paginated data by requesting all of the pages
#' and combining the results.
#'
#' @param filters    API filters. See the API documentations for 
#'                   additional information.
#'                   
#' @param structure  Structure parameter. See the API documentations 
#'                   for additional information.
#'                   
#' @return list      Comprehensive list of dictionaries containing all 
#'                   the data for the given ``filter`` and ``structure`.`
get_paginated_data <- function (filters, structure) {
  
    endpoint     <- "https://api.coronavirus.data.gov.uk/v1/data"
    results      <- list()
    current_page <- 1
    
    repeat {
        httr::GET(
            url   = endpoint,
            query = list(
                filters   = paste(filters, collapse = ";"),
                structure = jsonlite::toJSON(structure, auto_unbox = TRUE),
                page      = current_page
            ),
            timeout(10)
        ) -> response
        
        # Handle errors:
        if ( response$status_code >= 400 ) {
            err_msg = httr::http_status(response)
            stop(err_msg)
        } else if ( response$status_code == 204 ) {
            break
        }
        
        # Convert response from binary to JSON:
        json_text <- content(response, "text")
        dt        <- jsonlite::fromJSON(json_text)
        results   <- rbind(results, dt$data)
        
        if ( is.null( dt$pagination$`next` ) ){
            break
        }
        
        current_page <- current_page + 1;
    }
    
    return(results)
    
}
# Create filters:
query_filters <- c(
    "areaType=utla"
)
# Create the structure as a list or a list of lists:
query_structure <- list(
    date       = "date", 
    name       = "areaName", 
    code       = "areaCode", 
    daily      = "newCasesBySpecimenDate",
    cumulative = "cumCasesBySpecimenDate"
)
cases.upperLA <- get_paginated_data(query_filters, query_structure)
list(
  "Shape"                = dim(cases.upperLA),
  "Data (first 3 items)" = cases.upperLA[0:3, 0:-1]
) -> report
print(report)
# Rename columns for better distinction
cases.upperLA <- cases.upperLA %>%
  rename(
    daily.cases = daily,
    cumulative.cases = cumulative
  )
```

### Cases Lower-tier local authorities

Cases by specimen date

```{r}
#' Extracts paginated data by requesting all of the pages
#' and combining the results.
#'
#' @param filters    API filters. See the API documentations for 
#'                   additional information.
#'                   
#' @param structure  Structure parameter. See the API documentations 
#'                   for additional information.
#'                   
#' @return list      Comprehensive list of dictionaries containing all 
#'                   the data for the given ``filter`` and ``structure`.`
get_paginated_data <- function (filters, structure) {
  
    endpoint     <- "https://api.coronavirus.data.gov.uk/v1/data"
    results      <- list()
    current_page <- 1
    
    repeat {
        httr::GET(
            url   = endpoint,
            query = list(
                filters   = paste(filters, collapse = ";"),
                structure = jsonlite::toJSON(structure, auto_unbox = TRUE),
                page      = current_page
            ),
            timeout(10)
        ) -> response
        
        # Handle errors:
        if ( response$status_code >= 400 ) {
            err_msg = httr::http_status(response)
            stop(err_msg)
        } else if ( response$status_code == 204 ) {
            break
        }
        
        # Convert response from binary to JSON:
        json_text <- content(response, "text")
        dt        <- jsonlite::fromJSON(json_text)
        results   <- rbind(results, dt$data)
        
        if ( is.null( dt$pagination$`next` ) ){
            break
        }
        
        current_page <- current_page + 1;
    }
    
    return(results)
    
}
# Create filters:
query_filters <- c(
    "areaType=ltla"
)
# Create the structure as a list or a list of lists:
query_structure <- list(
    date       = "date", 
    name       = "areaName", 
    code       = "areaCode", 
    daily      = "newCasesBySpecimenDate",
    cumulative = "cumCasesBySpecimenDate"
)
cases.lowerLA <- get_paginated_data(query_filters, query_structure)
list(
  "Shape"                = dim(cases.lowerLA),
  "Data (first 3 items)" = cases.lowerLA[0:3, 0:-1]
) -> report
print(report)
# Rename columns for better distinction
cases.lowerLA <- cases.lowerLA %>%
  rename(
    daily.cases = daily,
    cumulative.cases = cumulative
  )
```

## Merge upper and lower tier cases

Right now, we have two data frames: one for lower-tier local authorities and one for upper-tier local authorities. We will merge them into one. 

```{r}
cases.alltiers <- rbind(cases.lowerLA, cases.upperLA)
```

### Remove duplicate rows

Some local authorities have been falling under lower-tier and upper-tier local authorities. We only want one entry per day for each LA. This is why we are removing duplicate rows using **distinct()**.

```{r}
cases.alltiers.distinct <- distinct(cases.alltiers)
```

## Rate of positive cases per 100k population  

We want to calculate the case rate per 100k population for each local authority. To be able to do this we need population estimates for each local authority.

### Import population estimates from mid 2019

Population estimated used are [Office for National Statistics 2019 mid-year estimates](https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/populationestimatesforukenglandandwalesscotlandandnorthernireland). 

Merging the data frames **cases.alltiers.distint** and **populations.estimates** at a later point will bring results for local authorities except for "Cornwall and Isles of Scilly" and "Hackney and City of London". Population estimates are given for Cornwall and Isles of Scilly individually. The same goes for Hackney and City of London. 

This is why I have added two new rows in the population estimates data frame before its import, manually combining the population numbers for "Cornwall and Isles of Scilly" and "Hackney and City of London", so that it matches the structure of the case data. 

```{r}
population.estimates <- read.csv("population estimates_mid2019-edited.csv", stringsAsFactors = FALSE)

#Rename column "code" for merging
population.estimates <- population.estimates %>%
  rename(code = Code, name = Name)
```

### Lookup the population for the regions

Note: In the population estimates data set from the ONS there are no population estimates for Aylesbury Vale, Chiltern, South Bucks, and Wycombe in Buckinghamshire. Instead they are all summarised under Buckinghamshire. This is why the **cases.alltiers.pop** data frame has less rows than the previous **cases.alltiers.distinct** data frame. 

```{r}
cases.alltiers.pop <- merge(cases.alltiers.distinct, population.estimates, by = "name")

#Dropping duplicate columns
cases.alltiers.pop <- cases.alltiers.pop[-c(5,6,7)]

#Rename column with populations estimates
cases.alltiers.pop <- cases.alltiers.pop %>%
  rename(
    population = All.ages,
    code = code.x
  )
```

### Calculate cases per 100k

Now that we have the daily cases and the population estimates for all areas we can calculate the case rate per 100k. 

We will divide daily cases by population and then multiply with 100,000. 

```{r}
cases.alltiers.pop$case.rate <- (cases.alltiers.pop$daily.cases/cases.alltiers.pop$population)*100000
```

## Calculating 7 day sum 

```{r}
cases.alltiers.sum <- cases.alltiers.pop %>%
  dplyr::arrange(desc(date)) %>% 
  group_by(name) %>%
  dplyr::mutate(cases_07da = zoo::rollsum(case.rate, k = 7, align="left", fill = NA)) %>%
  ungroup
```

## Export 

```{r}
write.csv(cases.alltiers.sum, file = "cases-LA-sum.csv")
```

# Case rates for UK

Cases by specimen date

```{r}
#' Extracts paginated data by requesting all of the pages
#' and combining the results.
#'
#' @param filters    API filters. See the API documentations for 
#'                   additional information.
#'                   
#' @param structure  Structure parameter. See the API documentations 
#'                   for additional information.
#'                   
#' @return list      Comprehensive list of dictionaries containing all 
#'                   the data for the given ``filter`` and ``structure`.`
get_paginated_data <- function (filters, structure) {
  
    endpoint     <- "https://api.coronavirus.data.gov.uk/v1/data"
    results      <- list()
    current_page <- 1
    
    repeat {
        httr::GET(
            url   = endpoint,
            query = list(
                filters   = paste(filters, collapse = ";"),
                structure = jsonlite::toJSON(structure, auto_unbox = TRUE),
                page      = current_page
            ),
            timeout(10)
        ) -> response
        
        # Handle errors:
        if ( response$status_code >= 400 ) {
            err_msg = httr::http_status(response)
            stop(err_msg)
        } else if ( response$status_code == 204 ) {
            break
        }
        
        # Convert response from binary to JSON:
        json_text <- content(response, "text")
        dt        <- jsonlite::fromJSON(json_text)
        results   <- rbind(results, dt$data)
        
        if ( is.null( dt$pagination$`next` ) ){
            break
        }
    
        current_page <- current_page + 1;
    }
    
    return(results)
    
}
# Create filters:
query_filters <- c(
    "areaType=nation"
)
# Create the structure as a list or a list of lists:
query_structure <- list(
    date       = "date", 
    name       = "areaName", 
    code       = "areaCode", 
    daily      = "newCasesBySpecimenDate",
    cumulative = "cumCasesBySpecimenDate"
)
cases.nations <- get_paginated_data(query_filters, query_structure)
list(
  "Shape"                = dim(cases.nations),
  "Data (first 3 items)" = cases.nations[0:3, 0:-1]
) -> report
print(report)
#Rename columns for better distinction
cases.nations <- cases.nations %>%
  rename(
    daily.cases.specimen = daily,
    cumulative.cases.specimen = cumulative
  )
```

## England, Scotland, Wales and Northern Ireland

### Lookup the population

The previously imported, edited file with the population estimates did not include the estimates for the nations.

```{r}
population.estimates <- read.csv("ukmidyearestimates20192020.csv", stringsAsFactors = FALSE)
```

```{r}
cases.nations.pop <- merge(cases.nations, population.estimates, by = "name")

#Dropping duplicate columns
cases.nations.pop <- cases.nations.pop[-c(5,6,7)]

#Rename column with populations estimates
cases.nations.pop <- cases.nations.pop %>%
  rename(
    population = All.ages,
  )
```

### Calculate cases per 100k

Now that we have the daily cases and the population estimates for all areas we can calculate the case rate per 100k. 

We will divide daily cases by population and then multiply with 100,000. 

```{r}
cases.nations$case.rate <- (cases.nations$daily.cases.specimen/cases.nations.pop$population)*100000
```

## Calculating 7 day sum 

```{r}
cases.nations.sum <- cases.nations %>%
  dplyr::arrange(desc(date)) %>% 
  group_by(name) %>%
  dplyr::mutate(cases_07da = zoo::rollsum(case.rate, k = 7, align="left", fill = NA)) %>%
  ungroup()
```

## Export 

```{r}
write.csv(cases.nations.sum, file = "cases-nations-sum.csv")
```
